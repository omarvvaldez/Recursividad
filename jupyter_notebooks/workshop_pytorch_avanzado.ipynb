{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3baf935",
      "metadata": {
        "id": "e3baf935"
      },
      "source": [
        "# Workshop PyTorch (Avanzado): Segmentación de Tumores Cerebrales con U-Net\n",
        "\n",
        "## Objetivo del Notebook\n",
        "En esta sesión práctica avanzada, construiremos un sistema de segmentación de imágenes médicas de principio a fin. Utilizaremos PyTorch para entrenar una red neuronal U-Net que identifique y delinee tumores en imágenes de resonancia magnética (MRI) cerebrales.\n",
        "\n",
        "**Aprenderás a:**\n",
        "- Implementar un pipeline de datos robusto con Dataset y DataLoader.\n",
        "- Utilizar Transfer Learning para acelerar el entrenamiento y mejorar el rendimiento.\n",
        "- Aprovechar librerías de alto nivel como segmentation-models-pytorch para no reinventar la rueda.\n",
        "- Definir métricas de evaluación específicas para segmentación, como el Dice Score.\n",
        "- Visualizar los resultados de forma intuitiva para interpretar el rendimiento del modelo.\n",
        "\n",
        "## Prerequisitos\n",
        "**Conocimientos:**\n",
        "- Fundamentos de Python y Programación Orientada a Objetos.\n",
        "- Conceptos básicos de Machine Learning.\n",
        "- Fundamentos de PyTorch (Tensores, nn.Module, bucle de entrenamiento básico).\n",
        "\n",
        "**Librerías necesarias:**\n",
        "- torch y torchvision\n",
        "- segmentation-models-pytorch\n",
        "- scikit-learn\n",
        "- numpy y matplotlib\n",
        "- Pillow (PIL)\n",
        "- tqdm (para barras de progreso)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f9f408",
      "metadata": {
        "id": "45f9f408"
      },
      "source": [
        "## Paso 1: Imports y Configuración Inicial\n",
        "Primero, importamos todas las librerías que necesitaremos y configuramos nuestro entorno. Es una buena práctica agrupar todas las importaciones al principio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b30de3",
      "metadata": {
        "id": "60b30de3"
      },
      "outputs": [],
      "source": [
        "# Celda 1: Imports y configuración\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- PyTorch y ecosistema ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# La librería mágica para modelos de segmentación\n",
        "# !pip install segmentation-models-pytorch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# --- Utilidades ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm # Para barras de progreso elegantes\n",
        "\n",
        "# Configuración de reproducibilidad y dispositivo\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e2591b",
      "metadata": {
        "id": "44e2591b"
      },
      "source": [
        "## Paso 2: Carga y Exploración de Datos\n",
        "\n",
        "Trabajaremos con el dataset LGG MRI Segmentation de Kaggle. Contiene imágenes de resonancia magnética de cerebros junto con máscaras segmentadas manualmente que indican la ubicación de gliomas de bajo grado (LGG).\n",
        "\n",
        "**Instrucciones de descarga:**\n",
        "- Descarga el dataset desde Kaggle.\n",
        "- Descomprímelo en una carpeta llamada `lgg-mri-segmentation` en el mismo directorio que este notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce7aa7ff",
      "metadata": {
        "id": "ce7aa7ff"
      },
      "outputs": [],
      "source": [
        "# Celda 2: Verificar ruta de datos y explorar archivos\n",
        "DATA_DIR = './lgg-mri-segmentation/kaggle_3m/'\n",
        "\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    print(f\"Error: El directorio '{DATA_DIR}' no fue encontrado.\")\n",
        "    print(\"Asegúrate de haber descargado y descomprimido los datos en la ruta correcta.\")\n",
        "else:\n",
        "    print(f\"Directorio de datos encontrado en: {DATA_DIR}\")\n",
        "    all_files = glob.glob(os.path.join(DATA_DIR, '*/*.tif'))\n",
        "    all_images = sorted([p for p in all_files if 'mask' not in p])\n",
        "    all_masks = sorted([p for p in all_files if 'mask' in p])\n",
        "    print(f\"Total de imágenes: {len(all_images)}\")\n",
        "    print(f\"Total de máscaras: {len(all_masks)}\")\n",
        "    if all_images:\n",
        "        print(f\"Ejemplo de ruta de imagen: {all_images[0]}\")\n",
        "    else:\n",
        "        print(\"No se encontraron imágenes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651945a6",
      "metadata": {
        "id": "651945a6"
      },
      "source": [
        "## Visualización de una imagen y su máscara\n",
        "\n",
        "Antes de construir el pipeline de datos, visualicemos una muestra del dataset para entender el formato de las imágenes y las máscaras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a46520",
      "metadata": {
        "id": "54a46520"
      },
      "outputs": [],
      "source": [
        "# Celda: Visualizar una imagen y su máscara correspondiente\n",
        "def show_image_and_mask(img_path: str, mask_path: str):\n",
        "    \"\"\"Muestra una imagen MRI y su máscara de segmentación lado a lado.\"\"\"\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    mask = Image.open(mask_path).convert(\"L\")\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    ax1.imshow(img)\n",
        "    ax1.set_title(\"Imagen MRI Original\")\n",
        "    ax1.axis('off')\n",
        "    ax2.imshow(mask, cmap='gray')\n",
        "    ax2.set_title(\"Máscara del Tumor (Ground Truth)\")\n",
        "    ax2.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Tomamos una muestra aleatoria para visualizar\n",
        "if 'all_images' in locals() and all_images:\n",
        "    sample_idx = random.randint(0, len(all_images) - 1)\n",
        "    sample_image_path = all_images[sample_idx]\n",
        "    sample_mask_path = sample_image_path.replace('.tif', '_mask.tif')\n",
        "    print(f\"Mostrando muestra #{sample_idx}\")\n",
        "    show_image_and_mask(sample_image_path, sample_mask_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c61e1d",
      "metadata": {
        "id": "45c61e1d"
      },
      "source": [
        "## Paso 3: El Pipeline de Datos de PyTorch (Dataset y DataLoader)\n",
        "\n",
        "**Teoría breve:**\n",
        "- `Dataset`: Clase abstracta de PyTorch para crear objetos que entienden nuestro conjunto de datos.\n",
        "- `DataLoader`: Encargado de agrupar los datos en batches, barajarlos y cargarlos en paralelo.\n",
        "\n",
        "A continuación, definimos las transformaciones necesarias para las imágenes y máscaras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3a9867",
      "metadata": {
        "id": "2e3a9867"
      },
      "outputs": [],
      "source": [
        "# Celda: Definir las transformaciones\n",
        "IMAGE_SIZE = 256\n",
        "transforms = T.Compose([\n",
        "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    T.ToTensor(), # Convierte la imagen a Tensor y normaliza los píxeles a [0, 1]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3695934c",
      "metadata": {
        "id": "3695934c"
      },
      "outputs": [],
      "source": [
        "# Celda: Crear la clase Dataset personalizada\n",
        "class BrainTumorDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset personalizado para cargar imágenes MRI de tumores cerebrales y sus máscaras.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_paths: list, transform: T.Compose = None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    def __getitem__(self, idx: int):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask_path = img_path.replace('.tif', '_mask.tif')\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "        mask = (mask > 0.5).float()\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e75d4a",
      "metadata": {
        "id": "72e75d4a"
      },
      "outputs": [],
      "source": [
        "# Celda: Dividir los datos y crear los DataLoaders\n",
        "train_paths, val_paths = train_test_split(all_images, test_size=0.2, random_state=SEED)\n",
        "\n",
        "train_dataset = BrainTumorDataset(train_paths, transform=transforms)\n",
        "val_dataset = BrainTumorDataset(val_paths, transform=transforms)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Tamaño del set de entrenamiento: {len(train_dataset)}\")\n",
        "print(f\"Tamaño del set de validación: {len(val_dataset)}\")\n",
        "print(f\"N° de lotes de entrenamiento: {len(train_loader)}\")\n",
        "print(f\"N° de lotes de validación: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f87f2fb",
      "metadata": {
        "id": "2f87f2fb"
      },
      "source": [
        "## Paso 4: El Modelo - U-Net y Transfer Learning\n",
        "\n",
        "**Teoría breve:**\n",
        "- **U-Net:** Arquitectura diseñada para segmentación biomédica, con encoder, decoder y skip connections.\n",
        "- **Transfer Learning:** Usamos un encoder preentrenado (por ejemplo, ResNet34) para aprovechar características aprendidas en ImageNet.\n",
        "- **Librería smp:** segmentation-models-pytorch nos permite crear U-Net con diferentes encoders fácilmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507e469a",
      "metadata": {
        "id": "507e469a"
      },
      "outputs": [],
      "source": [
        "# Celda: Crear el modelo U-Net con smp\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",        # Encoder backbone\n",
        "    encoder_weights=\"imagenet\",     # Transfer Learning\n",
        "    in_channels=3,                  # Imágenes RGB\n",
        "    classes=1,                      # Máscara binaria\n",
        ").to(DEVICE)\n",
        "\n",
        "# print(model)  # Descomenta para ver la arquitectura completa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9434b9a",
      "metadata": {
        "id": "d9434b9a"
      },
      "source": [
        "## Paso 5: Función de Pérdida y Optimizador\n",
        "\n",
        "**Teoría breve:**\n",
        "- **Dice Loss:** Métrica robusta para segmentación, ideal cuando hay desbalance entre fondo y objeto.\n",
        "- **Adam:** Optimizador recomendado para comenzar en tareas de segmentación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2778a0b5",
      "metadata": {
        "id": "2778a0b5"
      },
      "outputs": [],
      "source": [
        "# Celda: Definir la función de pérdida y el optimizador\n",
        "loss_fn = smp.losses.DiceLoss(mode=smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c30ae5",
      "metadata": {
        "id": "c4c30ae5"
      },
      "source": [
        "## Paso 6: Entrenamiento del Modelo\n",
        "\n",
        "**Teoría breve:**\n",
        "- El bucle de entrenamiento alterna entre fases de entrenamiento y validación.\n",
        "- Se calcula la pérdida y el Dice Score en cada época para monitorear el aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d2ef3a",
      "metadata": {
        "id": "57d2ef3a"
      },
      "outputs": [],
      "source": [
        "# Celda: Bucle de entrenamiento y validación\n",
        "EPOCHS = 1\n",
        "history = {'train_loss': [], 'val_loss': [], 'dice_score': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
        "    # --- Fase de Entrenamiento ---\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    for images, masks in tqdm(train_loader, desc=\"Entrenamiento\"):\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, masks)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss.item()\n",
        "    # --- Fase de Validación ---\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    epoch_dice_score = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=\"Validación\"):\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            epoch_val_loss += loss_fn(outputs, masks).item()\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            tp, fp, fn, tn = smp.metrics.get_stats(preds.long(), masks.long(), mode='binary')\n",
        "            dice_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction='micro-imagewise')\n",
        "            epoch_dice_score += dice_score\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    avg_dice_score = epoch_dice_score / len(val_loader)\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['dice_score'].append(avg_dice_score)\n",
        "    print(f\"Pérdida Entrenamiento: {avg_train_loss:.4f}\")\n",
        "    print(f\"Pérdida Validación:   {avg_val_loss:.4f}\")\n",
        "    print(f\"Dice Score Validación: {avg_dice_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04fbf8e",
      "metadata": {
        "id": "e04fbf8e"
      },
      "source": [
        "## Paso 7: Análisis de Resultados\n",
        "\n",
        "Un entrenamiento no está completo si no analizamos su rendimiento. Graficaremos la pérdida y el Dice Score a lo largo de las épocas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb82def",
      "metadata": {
        "id": "0eb82def"
      },
      "outputs": [],
      "source": [
        "# Celda: Graficar el historial de entrenamiento\n",
        "def plot_history(history: dict):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax1.plot(history['train_loss'], label='Pérdida Entrenamiento')\n",
        "    ax1.plot(history['val_loss'], label='Pérdida Validación')\n",
        "    ax1.set_title('Pérdida a lo largo de las Épocas')\n",
        "    ax1.set_xlabel('Época')\n",
        "    ax1.set_ylabel('Dice Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    ax2.plot(history['dice_score'], label='Dice Score Validación')\n",
        "    ax2.set_title('Dice Score a lo largo de las Épocas')\n",
        "    ax2.set_xlabel('Época')\n",
        "    ax2.set_ylabel('Dice Score')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88fe225f",
      "metadata": {
        "id": "88fe225f"
      },
      "source": [
        "## Visualización de Predicciones\n",
        "\n",
        "Mostramos la imagen original, la máscara real y la máscara predicha por el modelo para varias muestras del conjunto de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a49d561",
      "metadata": {
        "id": "4a49d561"
      },
      "outputs": [],
      "source": [
        "# Celda: Visualizar predicciones del modelo\n",
        "def visualize_predictions(model: nn.Module, loader: DataLoader, num_samples: int = 5):\n",
        "    model.eval()\n",
        "    samples_shown = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            images_np = images.cpu().numpy()\n",
        "            masks_np = masks.cpu().numpy()\n",
        "            preds_np = preds.cpu().numpy()\n",
        "            for i in range(images.size(0)):\n",
        "                if samples_shown >= num_samples:\n",
        "                    plt.show()\n",
        "                    return\n",
        "                fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
        "                img_display = np.transpose(images_np[i], (1, 2, 0))\n",
        "                ax1.imshow(img_display)\n",
        "                ax1.set_title(\"Input MRI\")\n",
        "                ax1.axis('off')\n",
        "                ax2.imshow(masks_np[i].squeeze(), cmap='gray')\n",
        "                ax2.set_title(\"Máscara Real (Ground Truth)\")\n",
        "                ax2.axis('off')\n",
        "                ax3.imshow(preds_np[i].squeeze(), cmap='gray')\n",
        "                ax3.set_title(\"Máscara Predicha por el Modelo\")\n",
        "                ax3.axis('off')\n",
        "                ax4.imshow(img_display)\n",
        "                ax4.imshow(preds_np[i].squeeze(), cmap='Reds', alpha=0.5)\n",
        "                ax4.set_title(\"Predicción Superpuesta\")\n",
        "                ax4.axis('off')\n",
        "                samples_shown += 1\n",
        "\n",
        "print(\"Mostrando predicciones en el conjunto de validación...\")\n",
        "visualize_predictions(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a7425bc",
      "metadata": {
        "id": "5a7425bc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DLRM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}